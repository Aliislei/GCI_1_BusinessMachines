#!/usr/bin/env python3
"""
é›¢è·ç‡ãƒ»ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã¨ã®ç›¸é–¢åˆ†æã‚¹ã‚¯ãƒªãƒ—ãƒˆ
GCIæœ€çµ‚èª²é¡Œ - æ©Ÿæ¢°å­¦ç¿’ã«ã‚ˆã‚‹äº‹æ¥­ææ¡ˆ
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
import japanize_matplotlib

def load_and_prepare_data():
    """ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã€å‰å‡¦ç†ã‚’è¡Œã†"""
    df = pd.read_csv('data/data.csv')
    
    # Attritionã‚’æ•°å€¤ã«å¤‰æ›ï¼ˆYes=1, No=0ï¼‰
    df['Attrition_numeric'] = df['Attrition'].apply(lambda x: 1 if x == 'Yes' else 0)
    
    return df

def encode_categorical_data(df):
    """ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’æ•°å€¤ã«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰"""
    df_encoded = df.copy()
    
    # ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«ã‚«ãƒ©ãƒ ã‚’ç‰¹å®š
    categorical_columns = df.select_dtypes(include=['object']).columns.tolist()
    
    # Attritionã¯æ—¢ã«å‡¦ç†æ¸ˆã¿ãªã®ã§é™¤å¤–
    if 'Attrition' in categorical_columns:
        categorical_columns.remove('Attrition')
    
    # Label Encodingã‚’å®Ÿè¡Œ
    le = LabelEncoder()
    for col in categorical_columns:
        df_encoded[f'{col}_encoded'] = le.fit_transform(df_encoded[col])
    
    return df_encoded, categorical_columns

def calculate_correlations(df, target_column):
    """æŒ‡å®šã—ãŸã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚«ãƒ©ãƒ ã¨ã®ç›¸é–¢ã‚’è¨ˆç®—"""
    # æ•°å€¤ã‚«ãƒ©ãƒ ã®ã¿ã‚’é¸æŠï¼ˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰æ¸ˆã¿ã‚«ãƒ©ãƒ ã‚‚å«ã‚€ï¼‰
    numeric_columns = df.select_dtypes(include=[np.number]).columns.tolist()
    
    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚«ãƒ©ãƒ ã‚’é™¤å¤–ï¼ˆè‡ªå·±ç›¸é–¢ã‚’é¿ã‘ã‚‹ãŸã‚ï¼‰
    if target_column in numeric_columns:
        numeric_columns.remove(target_column)
    
    # åˆ†æ•£ãŒ0ã®ã‚«ãƒ©ãƒ ã‚’é™¤å¤–ï¼ˆå…¨ã¦åŒã˜å€¤ã®ã‚«ãƒ©ãƒ ï¼‰
    valid_columns = []
    for col in numeric_columns:
        if df[col].var() > 0 and not df[col].isna().all():
            valid_columns.append(col)
    
    # ç›¸é–¢ã‚’è¨ˆç®—
    correlations = {}
    for col in valid_columns:
        corr = df[target_column].corr(df[col])
        if not pd.isna(corr):  # nanã§ãªã„å ´åˆã®ã¿è¿½åŠ 
            correlations[col] = corr
    
    # çµ¶å¯¾å€¤ã§ä¸¦ã³æ›¿ãˆï¼ˆç›¸é–¢ã®å¼·ã•ã§è©•ä¾¡ï¼‰
    correlations_sorted = dict(sorted(correlations.items(), 
                                    key=lambda x: abs(x[1]), reverse=True))
    
    return correlations_sorted

def display_correlation_results(correlations, target_name, top_n=15):
    """ç›¸é–¢åˆ†æçµæœã‚’è¡¨ç¤º"""
    print(f"=== {target_name}ã¨ã®ç›¸é–¢ãŒé«˜ã„ã‚«ãƒ©ãƒ  Top {top_n} ===")
    print("é †ä½ | ã‚«ãƒ©ãƒ å | ç›¸é–¢ä¿‚æ•°")
    print("-" * 50)
    
    for i, (col, corr) in enumerate(list(correlations.items())[:top_n], 1):
        print(f"{i:2d}ä½ | {col:<25} | {corr:7.4f}")
    
    print(f"\næ³¨ï¼šç›¸é–¢ä¿‚æ•°ã®ç¯„å›²ã¯ -1.0 ï½ 1.0")
    print(f"    æ­£ã®å€¤ï¼š{target_name}ã¨æ­£ã®ç›¸é–¢")
    print(f"    è² ã®å€¤ï¼š{target_name}ã¨è² ã®ç›¸é–¢")
    print(f"    çµ¶å¯¾å€¤ãŒå¤§ãã„ã»ã©ç›¸é–¢ãŒå¼·ã„")

def create_correlation_visualization(correlations, target_name, filename, top_n=15):
    """ç›¸é–¢åˆ†æçµæœã‚’å¯è¦–åŒ–"""
    top_cols = list(correlations.keys())[:top_n]
    top_corrs = [correlations[col] for col in top_cols]
    
    plt.figure(figsize=(14, 10))
    colors = ['red' if x > 0 else 'blue' for x in top_corrs]
    bars = plt.barh(range(len(top_cols)), top_corrs, color=colors, alpha=0.7)
    
    plt.yticks(range(len(top_cols)), 
               [col.replace('_encoded', '') for col in top_cols])
    plt.xlabel('ç›¸é–¢ä¿‚æ•°')
    plt.title(f'{target_name}ã¨ã®ç›¸é–¢ãŒé«˜ã„ã‚«ãƒ©ãƒ  Top {top_n}', fontsize=16, fontweight='bold')
    plt.grid(axis='x', alpha=0.3)
    
    # 0ã®ç·šã‚’è¿½åŠ 
    plt.axvline(x=0, color='black', linewidth=0.8, alpha=0.8)
    
    # ç›¸é–¢å€¤ã‚’ãƒãƒ¼ã«è¡¨ç¤º
    for i, (bar, corr) in enumerate(zip(bars, top_corrs)):
        plt.text(corr + (0.01 if corr > 0 else -0.01), i, 
                f'{corr:.3f}', ha='left' if corr > 0 else 'right', va='center')
    
    plt.tight_layout()
    plt.savefig(filename, dpi=300, bbox_inches='tight')
    plt.show()

def create_integrated_correlation_table(attrition_corr, performance_corr, stress_corr):
    """3ã¤ã®æŒ‡æ¨™ã®ç›¸é–¢ã‚’çµ±åˆã—ãŸè¡¨ã‚’ä½œæˆã—ã€fleetingã«ä¿å­˜"""
    import os
    from datetime import datetime
    
    # å…¨ã¦ã®ã‚«ãƒ©ãƒ ã‚’å–å¾—ï¼ˆé‡è¤‡ãªã—ï¼‰
    all_columns = set()
    all_columns.update(attrition_corr.keys())
    all_columns.update(performance_corr.keys()) 
    all_columns.update(stress_corr.keys())
    
    # ãƒ‡ãƒ¼ã‚¿ã®æ•´ç†
    integrated_data = []
    for col in sorted(all_columns):
        attrition_val = attrition_corr.get(col, 0.0)
        performance_val = performance_corr.get(col, 0.0)
        stress_val = stress_corr.get(col, 0.0)
        
        # çµ¶å¯¾å€¤ã®æœ€å¤§å€¤ã§ä¸¦ã³æ›¿ãˆç”¨ã®ã‚­ãƒ¼ã‚’ä½œæˆ
        max_abs_corr = max(abs(attrition_val), abs(performance_val), abs(stress_val))
        
        integrated_data.append({
            'column': col,
            'attrition': attrition_val,
            'performance': performance_val,
            'stress': stress_val,
            'max_abs': max_abs_corr
        })
    
    # æœ€å¤§çµ¶å¯¾å€¤ã§ã‚½ãƒ¼ãƒˆï¼ˆé™é †ï¼‰
    integrated_data.sort(key=lambda x: x['max_abs'], reverse=True)
    
    # Markdownãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆ
    os.makedirs('doc/fleeting', exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f'doc/fleeting/correlation_analysis_integrated_{timestamp}.md'
    
    with open(filename, 'w', encoding='utf-8') as f:
        f.write("# çµ±åˆç›¸é–¢åˆ†æè¡¨ - é›¢è·ç‡ãƒ»ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ»ã‚¹ãƒˆãƒ¬ã‚¹è©•ä¾¡\n\n")
        f.write(f"**ä½œæˆæ—¥æ™‚**: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}\n")
        f.write(f"**åˆ†æå¯¾è±¡**: {len(integrated_data)}å€‹ã®æ•°å€¤ã‚«ãƒ©ãƒ \n\n")
        
        f.write("## æ¦‚è¦\n")
        f.write("Iç¤¾äººäº‹ãƒ‡ãƒ¼ã‚¿ã«ãŠã‘ã‚‹å…¨æ•°å€¤ã‚«ãƒ©ãƒ ã¨ä»¥ä¸‹3æŒ‡æ¨™ã®ç›¸é–¢ä¿‚æ•°ã‚’ä¸€è¦§åŒ–ï¼š\n")
        f.write("- **é›¢è·ç‡ï¼ˆAttritionï¼‰**: é›¢è·=1, åœ¨è·=0\n") 
        f.write("- **ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ•°ï¼ˆPerformanceIndexï¼‰**: 30-100ã®ç¯„å›²\n")
        f.write("- **ã‚¹ãƒˆãƒ¬ã‚¹è©•ä¾¡ï¼ˆStressRatingï¼‰**: ã‚¹ãƒˆãƒ¬ã‚¹åº¦åˆã„è©•ä¾¡\n\n")
        
        f.write("## å®Œå…¨ç›¸é–¢è¡¨\n")
        f.write("| é †ä½ | ã‚«ãƒ©ãƒ å | é›¢è·ç‡ | ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ | ã‚¹ãƒˆãƒ¬ã‚¹è©•ä¾¡ | æœ€å¤§çµ¶å¯¾å€¤ |\n")
        f.write("|------|----------|:------:|:-------------:|:------------:|:----------:|\n")
        
        for i, data in enumerate(integrated_data, 1):
            # ã‚«ãƒ©ãƒ åã‚’é©åˆ‡ãªé•·ã•ã«èª¿æ•´
            column_name = data['column'].replace('_encoded', '').replace('_', '')[:20]
            f.write(f"| {i:2d} | {column_name:<20} | {data['attrition']:+6.3f} | {data['performance']:+6.3f} | {data['stress']:+6.3f} | {data['max_abs']:6.3f} |\n")
        
        f.write("\n## ç›¸é–¢å¼·åº¦åˆ†é¡\n")
        f.write("- **å¼·**: |r| â‰¥ 0.15\n")
        f.write("- **ä¸­**: 0.10 â‰¤ |r| < 0.15\n") 
        f.write("- **å¼±**: 0.05 â‰¤ |r| < 0.10\n")
        f.write("- **å¾®**: |r| < 0.05\n\n")
        
        # å„æŒ‡æ¨™åˆ¥ã®çµ±è¨ˆ
        f.write("## æŒ‡æ¨™åˆ¥çµ±è¨ˆ\n\n")
        
        for target_name, corr_dict in [
            ("é›¢è·ç‡", attrition_corr),
            ("ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ•°", performance_corr), 
            ("ã‚¹ãƒˆãƒ¬ã‚¹è©•ä¾¡", stress_corr)
        ]:
            strong = sum(1 for v in corr_dict.values() if abs(v) >= 0.15)
            medium = sum(1 for v in corr_dict.values() if 0.10 <= abs(v) < 0.15)
            weak = sum(1 for v in corr_dict.values() if 0.05 <= abs(v) < 0.10)
            micro = sum(1 for v in corr_dict.values() if abs(v) < 0.05)
            positive = sum(1 for v in corr_dict.values() if v > 0)
            negative = sum(1 for v in corr_dict.values() if v < 0)
            
            f.write(f"### {target_name}\n")
            f.write(f"- å¼·ã„ç›¸é–¢: {strong}å€‹\n")
            f.write(f"- ä¸­ç¨‹åº¦ç›¸é–¢: {medium}å€‹\n")
            f.write(f"- å¼±ã„ç›¸é–¢: {weak}å€‹\n")
            f.write(f"- å¾®å¼±ç›¸é–¢: {micro}å€‹\n")
            f.write(f"- æ­£ã®ç›¸é–¢: {positive}å€‹, è² ã®ç›¸é–¢: {negative}å€‹\n\n")
        
        # æ³¨ç›®ã™ã¹ãçŸ¥è¦‹
        f.write("## æ³¨ç›®ã™ã¹ãçŸ¥è¦‹\n\n")
        f.write("### æœ€ã‚‚å¼·ã„ç›¸é–¢ã‚’æŒã¤ã‚«ãƒ©ãƒ  Top 5\n")
        for i, data in enumerate(integrated_data[:5], 1):
            clean_name = data['column'].replace('_encoded', '').replace('_', ' ')
            f.write(f"{i}. **{clean_name}** (æœ€å¤§çµ¶å¯¾å€¤={data['max_abs']:.3f})\n")
            f.write(f"   - é›¢è·ç‡: {data['attrition']:+.3f}\n")
            f.write(f"   - ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹: {data['performance']:+.3f}\n") 
            f.write(f"   - ã‚¹ãƒˆãƒ¬ã‚¹: {data['stress']:+.3f}\n\n")
            
        # å¤šæ–¹é¢å½±éŸ¿è¦å› 
        f.write("### è¤‡æ•°æŒ‡æ¨™ã«å¼·ã„å½±éŸ¿ã‚’ä¸ãˆã‚‹è¦å› \n")
        multi_impact = []
        for data in integrated_data:
            strong_count = sum(1 for val in [data['attrition'], data['performance'], data['stress']] 
                             if abs(val) >= 0.10)
            if strong_count >= 2:
                multi_impact.append(data)
        
        if multi_impact:
            for data in multi_impact:
                clean_name = data['column'].replace('_encoded', '').replace('_', ' ')
                f.write(f"- **{clean_name}**: ")
                impacts = []
                if abs(data['attrition']) >= 0.10:
                    impacts.append(f"é›¢è·ç‡({data['attrition']:+.3f})")
                if abs(data['performance']) >= 0.10:
                    impacts.append(f"ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹({data['performance']:+.3f})")
                if abs(data['stress']) >= 0.10:
                    impacts.append(f"ã‚¹ãƒˆãƒ¬ã‚¹({data['stress']:+.3f})")
                f.write(", ".join(impacts) + "\n")
        else:
            f.write("è¤‡æ•°æŒ‡æ¨™ã«å¼·ã„å½±éŸ¿ã‚’ä¸ãˆã‚‹è¦å› ã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚\n")
            
        f.write(f"\n---\n*åˆ†æå®Ÿè¡Œæ—¥æ™‚: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}*\n")
    
    print(f"âœ… çµ±åˆç›¸é–¢åˆ†æè¡¨ã‚’ä½œæˆã—ã¾ã—ãŸ: {filename}")
    print(f"   ğŸ“Š åˆ†æå¯¾è±¡: {len(integrated_data)}å€‹ã®ã‚«ãƒ©ãƒ ")
    return filename

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("=== é›¢è·ç‡ãƒ»ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›¸é–¢åˆ†æ ===\n")
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    df = load_and_prepare_data()
    print(f"ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {df.shape}")
    print(f"é›¢è·ç‡: {df['Attrition_numeric'].mean():.3f}")
    print(f"ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ•°: å¹³å‡{df['PerformanceIndex'].mean():.1f} (ç¯„å›²: {df['PerformanceIndex'].min()}-{df['PerformanceIndex'].max()})\n")
    
    # ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
    df_encoded, categorical_cols = encode_categorical_data(df)
    print(f"ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã—ãŸã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«ã‚«ãƒ©ãƒ : {len(categorical_cols)}å€‹")
    print(f"ã‚«ãƒ©ãƒ å: {categorical_cols}\n")
    
    # 1. é›¢è·ç‡ã¨ã®ç›¸é–¢åˆ†æ
    print("=" * 60)
    print("ğŸ“ˆ 1. é›¢è·ç‡ï¼ˆAttritionï¼‰ã¨ã®ç›¸é–¢åˆ†æ")
    print("=" * 60)
    attrition_correlations = calculate_correlations(df_encoded, 'Attrition_numeric')
    
    # Top 15ã®è©³ç´°è¡¨ç¤º
    display_correlation_results(attrition_correlations, "é›¢è·ç‡")
    
    # å…¨æ•°å€¤ã‚«ãƒ©ãƒ ã®é †ä½ã¥ã‘è¡¨ç¤º
    print("\n" + "-" * 80)
    print("ğŸ“Š é›¢è·ç‡ã¨ã®ç›¸é–¢ - å…¨39ã‚«ãƒ©ãƒ å®Œå…¨ãƒ©ãƒ³ã‚­ãƒ³ã‚°")
    print("-" * 80)
    print("é †ä½ | ã‚«ãƒ©ãƒ å | ç›¸é–¢ä¿‚æ•° | åˆ†é¡")
    print("-" * 80)
    
    for i, (col, corr) in enumerate(attrition_correlations.items(), 1):
        # ç›¸é–¢ã®å¼·ã•ã‚’åˆ†é¡
        if abs(corr) >= 0.15:
            category = "ğŸ”´ å¼·"
        elif abs(corr) >= 0.10:
            category = "ğŸŸ¡ ä¸­"
        elif abs(corr) >= 0.05:
            category = "ğŸ”µ å¼±"
        else:
            category = "âšª å¾®"
            
        print(f"{i:2d}ä½ | {col:<25} | {corr:7.4f} | {category}")
    
    # çµ±è¨ˆã‚µãƒãƒªãƒ¼ã®è¨ˆç®—
    strong_corr = sum(1 for _, corr in attrition_correlations.items() if abs(corr) >= 0.15)
    medium_corr = sum(1 for _, corr in attrition_correlations.items() if 0.10 <= abs(corr) < 0.15)
    weak_corr = sum(1 for _, corr in attrition_correlations.items() if 0.05 <= abs(corr) < 0.10)
    micro_corr = sum(1 for _, corr in attrition_correlations.items() if abs(corr) < 0.05)
    
    positive_corr = sum(1 for _, corr in attrition_correlations.items() if corr > 0)
    negative_corr = sum(1 for _, corr in attrition_correlations.items() if corr < 0)
    
    max_corr = max(attrition_correlations.values())
    min_corr = min(attrition_correlations.values())
    avg_corr = sum(attrition_correlations.values()) / len(attrition_correlations)
    
    print(f"\nğŸ“Š çµ±è¨ˆã‚µãƒãƒªãƒ¼:")
    print(f"  â€¢ ç·æ•°å€¤ã‚«ãƒ©ãƒ æ•°: {len(attrition_correlations)}å€‹")
    print(f"  â€¢ å¼·ã„ç›¸é–¢(|r|â‰¥0.15): {strong_corr}å€‹ ({strong_corr/len(attrition_correlations)*100:.1f}%)")
    print(f"  â€¢ ä¸­ç¨‹åº¦ç›¸é–¢(0.10â‰¤|r|<0.15): {medium_corr}å€‹ ({medium_corr/len(attrition_correlations)*100:.1f}%)")
    print(f"  â€¢ å¼±ã„ç›¸é–¢(0.05â‰¤|r|<0.10): {weak_corr}å€‹ ({weak_corr/len(attrition_correlations)*100:.1f}%)")
    print(f"  â€¢ å¾®å¼±ç›¸é–¢(|r|<0.05): {micro_corr}å€‹ ({micro_corr/len(attrition_correlations)*100:.1f}%)")
    print(f"  â€¢ æ­£ã®ç›¸é–¢: {positive_corr}å€‹, è² ã®ç›¸é–¢: {negative_corr}å€‹")
    print(f"  â€¢ æœ€å¤§ç›¸é–¢: {max_corr:.4f}, æœ€å°ç›¸é–¢: {min_corr:.4f}, å¹³å‡: {avg_corr:.4f}")
    
    print("\nåˆ†é¡åŸºæº–: å¼·(|r|â‰¥0.15), ä¸­(0.10â‰¤|r|<0.15), å¼±(0.05â‰¤|r|<0.10), å¾®(|r|<0.05)")
    
    # Top 15ã®å¯è¦–åŒ–
    create_correlation_visualization(attrition_correlations, "é›¢è·ç‡", 
                                   'src/attrition_correlation_top15.png')
    
    print("\n" + "=" * 60)
    print("ğŸ¯ 2. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ•°ï¼ˆPerformanceIndexï¼‰ã¨ã®ç›¸é–¢åˆ†æ")
    print("=" * 60)
    performance_correlations = calculate_correlations(df_encoded, 'PerformanceIndex')
    display_correlation_results(performance_correlations, "ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ•°")
    create_correlation_visualization(performance_correlations, "ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ•°", 
                                   'src/performance_correlation_top15.png')
    
    print("\n" + "=" * 60)
    print("ğŸ˜° 3. ã‚¹ãƒˆãƒ¬ã‚¹è©•ä¾¡ï¼ˆStressRatingï¼‰ã¨ã®ç›¸é–¢åˆ†æ")
    print("=" * 60)
    stress_correlations = calculate_correlations(df_encoded, 'StressRating')
    display_correlation_results(stress_correlations, "ã‚¹ãƒˆãƒ¬ã‚¹è©•ä¾¡")
    create_correlation_visualization(stress_correlations, "ã‚¹ãƒˆãƒ¬ã‚¹è©•ä¾¡", 
                                   'src/stress_correlation_top15.png')
    
    # 4. çµ±åˆç›¸é–¢åˆ†æè¡¨ã®ä½œæˆãƒ»ä¿å­˜
    print("\n" + "=" * 60)
    print("ğŸ“‹ 4. çµ±åˆç›¸é–¢åˆ†æè¡¨ã®ä½œæˆãƒ»ä¿å­˜")
    print("=" * 60)
    create_integrated_correlation_table(attrition_correlations, performance_correlations, stress_correlations)
    
    # 5. æ¯”è¼ƒåˆ†æ
    print("\n" + "=" * 60)
    print("ğŸ” 5. é›¢è·ç‡ vs ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ - é‡è¦è¦å› æ¯”è¼ƒ")
    print("=" * 60)
    
    # ä¸Šä½10è¦å› ã‚’æ¯”è¼ƒ
    attrition_top10 = list(attrition_correlations.items())[:10]
    performance_top10 = list(performance_correlations.items())[:10]
    
    print("é›¢è·ç‡ã«å½±éŸ¿ã™ã‚‹è¦å›  Top 10:")
    for i, (col, corr) in enumerate(attrition_top10, 1):
        print(f"  {i:2d}. {col:<25} ({corr:+.3f})")
    
    print("\nãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã«å½±éŸ¿ã™ã‚‹è¦å›  Top 10:")
    for i, (col, corr) in enumerate(performance_top10, 1):
        print(f"  {i:2d}. {col:<25} ({corr:+.3f})")
    
    # å…±é€šè¦å› ã®åˆ†æ
    attrition_factors = set([col for col, _ in attrition_top10])
    performance_factors = set([col for col, _ in performance_top10])
    common_factors = attrition_factors.intersection(performance_factors)
    
    print(f"\nğŸ¤ ä¸¡æ–¹ã«å½±éŸ¿ã™ã‚‹å…±é€šè¦å›  ({len(common_factors)}å€‹):")
    if common_factors:
        for factor in common_factors:
            attrition_corr = attrition_correlations[factor]
            performance_corr = performance_correlations[factor]
            print(f"  â€¢ {factor:<25} | é›¢è·ç‡: {attrition_corr:+.3f} | ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹: {performance_corr:+.3f}")
    else:
        print("  â€¢ å…±é€šè¦å› ã¯ã‚ã‚Šã¾ã›ã‚“ï¼ˆTop 10ç¯„å›²å†…ï¼‰")
    
    return {
        'attrition_correlations': attrition_correlations,
        'performance_correlations': performance_correlations,
        'stress_correlations': stress_correlations
    }

if __name__ == "__main__":
    results = main() 